{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "04b3dd3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets.hand_pose_dataset import HandPoseDataset\n",
    "from os.path import join, isdir\n",
    "from os import listdir\n",
    "import yaml\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d13ef3a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{tabular}{lrlllllllll}\n",
      "\\toprule\n",
      " & Normalize landmarks & Landmarks backbone & Image backbone & Accuracy & F1 & Precision & Recall & Loss & # Params (k) & Time (ms) \\\\\n",
      "\\midrule\n",
      "0 & True & mlp & NaN & 90.887 ± 3.419 & 90.599 ± 3.59 & 91.689 ± 3.067 & 90.887 ± 3.419 & 81.822 ± 6.904 & 4177.0 ± 0.0 & 0.381 ± 0.017 \\\\\n",
      "1 & False & mlp & NaN & 79.678 ± 5.33 & 79.661 ± 5.205 & 83.249 ± 4.14 & 79.678 ± 5.33 & 115.922 ± 11.002 & 4177.0 ± 0.0 & 0.376 ± 0.025 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "checkpoints_path = join(\"checkpoints\", \"results\")\n",
    "subject_ids = HandPoseDataset._get_subject_ids()\n",
    "data = {}\n",
    "metrics = []\n",
    "for checkpoint_name in sorted(listdir(checkpoints_path)):\n",
    "    # check if the checkpoint has complete experiments\n",
    "    subjects_in_checkpoint = []\n",
    "    for folder in listdir(join(checkpoints_path, checkpoint_name)):\n",
    "        if folder in subject_ids and isdir(join(checkpoints_path, checkpoint_name, folder)) and \"metrics.yaml\" in listdir(join(checkpoints_path, checkpoint_name, folder)):\n",
    "            subjects_in_checkpoint.append(folder)\n",
    "    # skips empty checkpoints\n",
    "    if len(subjects_in_checkpoint) <= 1:\n",
    "        continue\n",
    "    # loads the cfg used\n",
    "    with open(join(checkpoints_path, checkpoint_name, \"cfg.yaml\")) as f:\n",
    "        cfg = yaml.safe_load(f)\n",
    "    # initialize the data dict for this checkpoint\n",
    "    metrics_per_run = []\n",
    "    # loops over each subject\n",
    "    for subject_id in subjects_in_checkpoint:\n",
    "        # load the metrics for this subject\n",
    "        with open(\n",
    "            join(checkpoints_path, checkpoint_name, subject_id, \"metrics.yaml\")\n",
    "        ) as f:\n",
    "            metrics_per_run.append(yaml.safe_load(f)[0])\n",
    "    # compute the mean metrics for this checkpoint\n",
    "    metrics_per_run = pd.DataFrame(metrics_per_run)\n",
    "    metrics_per_run_mean, metrics_per_run_std = (\n",
    "        metrics_per_run.mean(),\n",
    "        metrics_per_run.std(),\n",
    "    )\n",
    "    # parses some numbers\n",
    "    for col in metrics_per_run.columns:\n",
    "        for df in [metrics_per_run_mean, metrics_per_run_std]:\n",
    "            if col.startswith(\"cls_\"):\n",
    "                df[col] = (df[col] * 100).round(3)\n",
    "            elif col.startswith(\"num_params\"):\n",
    "                df[col] = (df[col] / 1e3).astype(int)\n",
    "            elif col.startswith(\"time_test\"):\n",
    "                df[col] = (df[col] * 1e3).round(3)\n",
    "    metrics_per_run = (\n",
    "        metrics_per_run_mean.astype(str) + \" ± \" + metrics_per_run_std.astype(str)\n",
    "    )\n",
    "    metrics_per_run[\"normalize_landmarks\"] = cfg[\"normalize_landmarks\"]\n",
    "    metrics_per_run[\"image_backbone\"] = cfg[\"image_backbone_name\"]\n",
    "    metrics_per_run[\"landmarks_backbone\"] = cfg[\"landmarks_backbone_name\"]\n",
    "    # append to the metrics list\n",
    "    metrics.append(metrics_per_run)\n",
    "\n",
    "# parses the metrics into a DataFrame\n",
    "metrics = pd.DataFrame(metrics)\n",
    "metrics = metrics[\n",
    "    [\n",
    "        \"normalize_landmarks\",\n",
    "        \"landmarks_backbone\",\n",
    "        \"image_backbone\",\n",
    "        \"cls_acc_test\",\n",
    "        \"cls_f1_test\",\n",
    "        \"cls_prec_test\",\n",
    "        \"cls_rec_test\",\n",
    "        \"cls_loss_test\",\n",
    "        \"num_params_test\",\n",
    "        \"time_test\",\n",
    "    ]\n",
    "]\n",
    "metrics = metrics.rename(\n",
    "    columns={\n",
    "        \"normalize_landmarks\": \"Normalize landmarks\",\n",
    "        \"image_backbone\": \"Image backbone\",\n",
    "        \"landmarks_backbone\": \"Landmarks backbone\",\n",
    "        \"cls_acc_test\": \"Accuracy\",\n",
    "        \"cls_f1_test\": \"F1\",\n",
    "        \"cls_prec_test\": \"Precision\",\n",
    "        \"cls_rec_test\": \"Recall\",\n",
    "        \"cls_loss_test\": \"Loss\",\n",
    "        \"num_params_test\": \"# Params (k)\",\n",
    "        \"time_test\": \"Time (ms)\",\n",
    "    }\n",
    ")\n",
    "metrics = metrics.sort_values(by=[\"Landmarks backbone\", \"Image backbone\"])\n",
    "metrics = metrics.reset_index(drop=True)\n",
    "print(metrics.to_latex())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6acef8e5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27d1fc4f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml2hp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
